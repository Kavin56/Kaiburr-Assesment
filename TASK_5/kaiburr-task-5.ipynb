{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:16:14.619825Z",
     "iopub.status.busy": "2025-10-18T15:16:14.619502Z",
     "iopub.status.idle": "2025-10-18T15:16:19.318781Z",
     "shell.execute_reply": "2025-10-18T15:16:19.317608Z",
     "shell.execute_reply.started": "2025-10-18T15:16:14.619805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:16:27.151927Z",
     "iopub.status.busy": "2025-10-18T15:16:27.151622Z",
     "iopub.status.idle": "2025-10-18T15:16:29.260386Z",
     "shell.execute_reply": "2025-10-18T15:16:29.259697Z",
     "shell.execute_reply.started": "2025-10-18T15:16:27.151899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re, string, nltk, emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd, numpy as np, re, nltk, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:16:29.261825Z",
     "iopub.status.busy": "2025-10-18T15:16:29.261432Z",
     "iopub.status.idle": "2025-10-18T15:18:44.061370Z",
     "shell.execute_reply": "2025-10-18T15:18:44.060537Z",
     "shell.execute_reply.started": "2025-10-18T15:16:29.261805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "url = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\"\n",
    "df = pd.read_csv(url, compression=\"zip\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:44.062761Z",
     "iopub.status.busy": "2025-10-18T15:18:44.062512Z",
     "iopub.status.idle": "2025-10-18T15:18:46.711088Z",
     "shell.execute_reply": "2025-10-18T15:18:46.710266Z",
     "shell.execute_reply.started": "2025-10-18T15:18:44.062742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def categorize_product(product):\n",
    "    if 'Credit reporting' in product or 'Credit repair' in product:\n",
    "        return 'Credit reporting, repair, or other'\n",
    "    elif product == 'Debt collection':\n",
    "        return 'Debt collection'\n",
    "    elif product == 'Consumer Loan':\n",
    "        return 'Consumer Loan'\n",
    "    elif product == 'Mortgage':\n",
    "        return 'Mortgage'\n",
    "    else:\n",
    "        return 'Credit reporting, repair, or other' # Group all other categories into the 0th class\n",
    "\n",
    "df['Product'] = df['Product'].apply(categorize_product)\n",
    "\n",
    "# Display the count of each product category after re-categorization\n",
    "print(df[\"Product\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:46.712272Z",
     "iopub.status.busy": "2025-10-18T15:18:46.711988Z",
     "iopub.status.idle": "2025-10-18T15:18:46.720989Z",
     "shell.execute_reply": "2025-10-18T15:18:46.720270Z",
     "shell.execute_reply.started": "2025-10-18T15:18:46.712246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:46.722427Z",
     "iopub.status.busy": "2025-10-18T15:18:46.722261Z",
     "iopub.status.idle": "2025-10-18T15:18:59.138901Z",
     "shell.execute_reply": "2025-10-18T15:18:59.138114Z",
     "shell.execute_reply.started": "2025-10-18T15:18:46.722414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.strip().lower())\n",
    "df = df[[\"product\", \"consumer complaint narrative\"]].dropna()\n",
    "\n",
    "categories = [\n",
    "    \"Credit reporting, repair, or other\",\n",
    "    \"Debt collection\",\n",
    "    \"Consumer Loan\",\n",
    "    \"Mortgage\"\n",
    "]\n",
    "\n",
    "df = df[df[\"product\"].isin(categories)]\n",
    "df = df.rename(columns={\"consumer complaint narrative\": \"text\"}).sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Filtered dataset size: {len(df)}\")\n",
    "print(df[\"product\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:59.140243Z",
     "iopub.status.busy": "2025-10-18T15:18:59.139846Z",
     "iopub.status.idle": "2025-10-18T15:18:59.630095Z",
     "shell.execute_reply": "2025-10-18T15:18:59.629240Z",
     "shell.execute_reply.started": "2025-10-18T15:18:59.140216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:59.631233Z",
     "iopub.status.busy": "2025-10-18T15:18:59.630967Z",
     "iopub.status.idle": "2025-10-18T15:18:59.638120Z",
     "shell.execute_reply": "2025-10-18T15:18:59.637539Z",
     "shell.execute_reply.started": "2025-10-18T15:18:59.631208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re, string, emoji, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize objects\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Add extra common or dataset-specific stopwords\n",
    "custom_stopwords = {\n",
    "    \"xxxx\", \"xx\", \"na\", \"nan\", \"n/a\", \"account\", \"creditor\", \"loan\", \"report\",\n",
    "    \"complaint\", \"consumer\", \"finance\", \"company\"\n",
    "}\n",
    "stop_words |= custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:59.639136Z",
     "iopub.status.busy": "2025-10-18T15:18:59.638870Z",
     "iopub.status.idle": "2025-10-18T15:18:59.651840Z",
     "shell.execute_reply": "2025-10-18T15:18:59.651281Z",
     "shell.execute_reply.started": "2025-10-18T15:18:59.639114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text_advanced(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes consumer complaint text.\n",
    "    Steps:\n",
    "    1️⃣ Lowercasing\n",
    "    2️⃣ Removing URLs, emails, numbers, HTML tags\n",
    "    3️⃣ Expanding contractions (can't → cannot)\n",
    "    4️⃣ Removing punctuation, emojis, and extra spaces\n",
    "    5️⃣ Lemmatizing + removing stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove URLs, emails, HTML tags, and numbers\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S*@\\S*\\s?\", \" \", text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # Expand common contractions\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "        \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for k, v in contractions.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # Remove emojis and punctuation\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize + remove stopwords and short tokens\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(tok) for tok in tokens\n",
    "        if tok not in stop_words and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    # Join back into single string\n",
    "    return \" \".join(clean_tokens).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:59.652826Z",
     "iopub.status.busy": "2025-10-18T15:18:59.652569Z",
     "iopub.status.idle": "2025-10-18T15:18:59.703784Z",
     "shell.execute_reply": "2025-10-18T15:18:59.702996Z",
     "shell.execute_reply.started": "2025-10-18T15:18:59.652805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:18:59.704873Z",
     "iopub.status.busy": "2025-10-18T15:18:59.704612Z",
     "iopub.status.idle": "2025-10-18T16:42:36.258844Z",
     "shell.execute_reply": "2025-10-18T16:42:36.257992Z",
     "shell.execute_reply.started": "2025-10-18T15:18:59.704856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    tokenizer = word_tokenize\n",
    "except LookupError:\n",
    "    print(\"punkt tokenizer not found, using simple split() instead.\")\n",
    "    tokenizer = lambda x: x.split()\n",
    "\n",
    "# Update function to use fallback tokenizer\n",
    "def clean_text_final(text):\n",
    "    # Use your advanced cleaning steps\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S*@\\S*\\s?\", \" \", text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "        \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\",\n",
    "        \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for k, v in contractions.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words and len(tok) > 2]\n",
    "    return \" \".join(clean_tokens).strip()\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning text (this may take a minute)...\")\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_final)\n",
    "\n",
    "# Show sample results\n",
    "print(df[[\"text\", \"clean_text\"]].sample(3, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:42:36.261171Z",
     "iopub.status.busy": "2025-10-18T16:42:36.260940Z",
     "iopub.status.idle": "2025-10-18T16:50:08.043973Z",
     "shell.execute_reply": "2025-10-18T16:50:08.043241Z",
     "shell.execute_reply.started": "2025-10-18T16:42:36.261152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # unigrams + bigrams\n",
    "X = tfidf.fit_transform(df[\"clean_text\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "print(\"TF-IDF vectorization completed. Feature matrix shape:\", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:50:08.045375Z",
     "iopub.status.busy": "2025-10-18T16:50:08.045155Z",
     "iopub.status.idle": "2025-10-18T16:50:26.050772Z",
     "shell.execute_reply": "2025-10-18T16:50:26.050169Z",
     "shell.execute_reply.started": "2025-10-18T16:50:08.045359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:50:26.051996Z",
     "iopub.status.busy": "2025-10-18T16:50:26.051519Z",
     "iopub.status.idle": "2025-10-18T16:54:10.742344Z",
     "shell.execute_reply": "2025-10-18T16:54:10.741601Z",
     "shell.execute_reply.started": "2025-10-18T16:50:26.051978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_words = 5000  # Maximum number of words to keep\n",
    "maxlen = 100  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:54:10.744526Z",
     "iopub.status.busy": "2025-10-18T16:54:10.744031Z",
     "iopub.status.idle": "2025-10-18T16:54:27.433473Z",
     "shell.execute_reply": "2025-10-18T16:54:27.432878Z",
     "shell.execute_reply.started": "2025-10-18T16:54:10.744506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Split data for RNN\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(\n",
    "    padded_sequences, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:54:27.434457Z",
     "iopub.status.busy": "2025-10-18T16:54:27.434203Z",
     "iopub.status.idle": "2025-10-18T16:54:28.456813Z",
     "shell.execute_reply": "2025-10-18T16:54:28.455956Z",
     "shell.execute_reply.started": "2025-10-18T16:54:27.434433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(max_words, 64, input_length=maxlen),\n",
    "    SimpleRNN(64),\n",
    "    Dense(len(le.classes_), activation='softmax') # Output layer with number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:54:28.457958Z",
     "iopub.status.busy": "2025-10-18T16:54:28.457676Z",
     "iopub.status.idle": "2025-10-18T16:54:28.483669Z",
     "shell.execute_reply": "2025-10-18T16:54:28.483122Z",
     "shell.execute_reply.started": "2025-10-18T16:54:28.457920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T16:56:19.467713Z",
     "iopub.status.busy": "2025-10-18T16:56:19.467301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = rnn_model.fit(X_train_rnn, y_train_rnn,\n",
    "                        epochs=5,\n",
    "                        batch_size=32,\n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T16:56:15.942798Z",
     "iopub.status.idle": "2025-10-18T16:56:15.943093Z",
     "shell.execute_reply": "2025-10-18T16:56:15.942934Z",
     "shell.execute_reply.started": "2025-10-18T16:56:15.942918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = rnn_model.evaluate(X_test_rnn, y_test_rnn, verbose=0)\n",
    "print(f\"RNN Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T16:56:15.944286Z",
     "iopub.status.idle": "2025-10-18T16:56:15.944498Z",
     "shell.execute_reply": "2025-10-18T16:56:15.944409Z",
     "shell.execute_reply.started": "2025-10-18T16:56:15.944400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict and display classification report\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test_rnn), axis=-1)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_rnn, y_pred_rnn, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T16:56:15.945357Z",
     "iopub.status.idle": "2025-10-18T16:56:15.945620Z",
     "shell.execute_reply": "2025-10-18T16:56:15.945522Z",
     "shell.execute_reply.started": "2025-10-18T16:56:15.945509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_rnn = confusion_matrix(y_test_rnn, y_pred_rnn)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_rnn, display_labels=le.classes_).plot(\n",
    "    cmap=\"Blues\", xticks_rotation=45\n",
    ")\n",
    "plt.title(\"Confusion Matrix – Simple RNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters for tokenization and padding\n",
    "max_words = 5000  # Maximum number of words to keep\n",
    "maxlen = 100      # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Split data for BiLSTM\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the BiLSTM model\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(max_words, 64, input_length=maxlen),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(len(le.classes_), activation='softmax')  # Output layer with number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "bilstm_model.compile(optimizer='adam',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = bilstm_model.fit(X_train, y_train,\n",
    "                           epochs=5,\n",
    "                           batch_size=32,\n",
    "                           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = bilstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"BiLSTM Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict and display classification report\n",
    "y_pred = np.argmax(bilstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_).plot(\n",
    "    cmap=\"Blues\", xticks_rotation=45\n",
    ")\n",
    "plt.title(\"Confusion Matrix – BiLSTM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for tokenization\n",
    "maxlen = 100  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "X_tokens = tokenizer(list(df[\"clean_text\"]), padding=True, truncation=True, max_length=maxlen, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data for BERT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tokens['input_ids'], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the data into tf.data.Dataset format\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the BERT model for sequence classification\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "bert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),  # Use smaller learning rate for BERT\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model with early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "history = bert_model.fit(\n",
    "    train_data.batch(32),\n",
    "    epochs=5,\n",
    "    validation_data=test_data.batch(32),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = bert_model.evaluate(test_data.batch(32), verbose=0)\n",
    "print(f\"BERT Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict and display classification report\n",
    "y_pred = np.argmax(bert_model.predict(X_test.batch(32)).logits, axis=-1)\n",
    "print(\"\\n📈 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df[\"label\"].unique()).plot(\n",
    "    cmap=\"Blues\", xticks_rotation=45\n",
    ")\n",
    "plt.title(\"Confusion Matrix – BERT\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
